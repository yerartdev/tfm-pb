{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gerardo de Miguel González"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** The aim of this notebook is to find a basic workflow for doing machine learning classification of an imbalanced dataset using best practices taken from other people's examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification Proof of Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** These ones:\n",
    "\n",
    "  - [STHDA](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/) Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net by kassambara -- 11/03/2018 from  [Articles](http://www.sthda.com/english/articles/36-classification-methods-essentials/) - Classification Methods Essentials\n",
    "  - [Stanford](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#log) Glmnet Vignette: Logistic Regression by [Trevor Hastie](https://web.stanford.edu/~hastie/) and Junyang Qian, Stanford June 26, 2014\n",
    "  - [Lab Notebook](http://homepages.uc.edu/~lis6/Teaching/DM18Spring/Lab/lab5_logit.html) in [Data Mining I (BANA7046) 2018 Spring](http://homepages.uc.edu/~lis6/DM-18Spring.html) by [Shaobo Li](http://homepages.uc.edu/~lis6/), see [Variable Selection section](http://homepages.uc.edu/~lis6/Teaching/DM18Spring/Lab/lab5_logit.html#variable-selection).\n",
    "  \n",
    "**::NOTE::** The LASSO is not very good at handling variables which show correlation between them and thus can sometimes show very wild behaviors. (From [rstatisticsblog](https://www.rstatisticsblog.com/data-science-in-action/lasso-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Helper function to install missing libraries before loading them\n",
    "# https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them\n",
    "using<-function(...) {\n",
    "    libs<-unlist(list(...))\n",
    "    req<-unlist(lapply(libs,require,character.only=TRUE))\n",
    "    need<-libs[req==FALSE]\n",
    "    if(length(need)>0){ \n",
    "        install.packages(need)\n",
    "        lapply(need,require,character.only=TRUE)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::Downsizing to simpler packages\n",
    "#using('tidyverse','caret','glmnet', 'mlbench')\n",
    "# ... repr to fix size of graphics within jupyter noteboook  ...\n",
    "# https://blog.revolutionanalytics.com/2015/09/resizing-plots-in-the-r-kernel-for-jupyter-notebooks.html\n",
    "# ... [NO] crossval: Generic Functions for Cross Validation ...\n",
    "#     Contains generic functions for performing cross validation and for computing diagnostic errors ...\n",
    "# https://cran.r-project.org/web/packages/crossval/\n",
    "# https://cran.r-project.org/web/packages/crossval/crossval.pdf\n",
    "# ... plotmo: Plot a Model's Residuals, Response, and Partial Dependence Plots ...\n",
    "#       Plot model surfaces for a wide variety of models using partial dependence plots and other techniques. \n",
    "#       Also plot model residuals and other information on the model.\n",
    "# https://cran.r-project.org/web/packages/plotmo/index.html\n",
    "# https://cran.r-project.org/web/packages/plotmo/plotmo.pdf\n",
    "# ... MLmetrics: Machine Learning Evaluation Metrics\n",
    "#     A collection of evaluation metrics, including loss, score and utility functions, that measure regression, \n",
    "#     classification and ranking performance.\n",
    "using('repr', 'mlbench', 'caTools', 'glmnet', 'plotmo', 'MLmetrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** `crossval` package first install output here:\n",
    "\n",
    "```R\n",
    "Loading required package: mlbench\n",
    "Loading required package: caTools\n",
    "Loading required package: glmnet\n",
    "Loading required package: Matrix\n",
    "Loading required package: foreach\n",
    "Loaded glmnet 2.0-18\n",
    "\n",
    "Loading required package: crossval\n",
    "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
    "“there is no package called ‘crossval’”Installing package into ‘/home/jovyan/R/x86_64-pc-linux-gnu-library/3.6’\n",
    "(as ‘lib’ is unspecified)\n",
    "Loading required package: crossval\n",
    "  1. TRUE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** `tidyverse` output first install here:\n",
    "\n",
    "```R\n",
    "Loading required package: tidyverse\n",
    "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
    "“there is no package called ‘tidyverse’”Loading required package: caret\n",
    "Loading required package: lattice\n",
    "Loading required package: ggplot2\n",
    "Loading required package: glmnet\n",
    "Loading required package: Matrix\n",
    "Loading required package: foreach\n",
    "Loaded glmnet 2.0-18\n",
    "\n",
    "Installing package into ‘/home/jovyan/R/x86_64-pc-linux-gnu-library/3.6’\n",
    "(as ‘lib’ is unspecified)\n",
    "also installing the dependencies ‘tinytex’, ‘rmarkdown’, ‘selectr’, ‘broom’, ‘dbplyr’, ‘modelr’, ‘reprex’, ‘rvest’\n",
    "\n",
    "Loading required package: tidyverse\n",
    "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
    "✔ tibble  2.1.3     ✔ purrr   0.3.2\n",
    "✔ tidyr   0.8.3     ✔ dplyr   0.8.3\n",
    "✔ readr   1.3.1     ✔ stringr 1.4.0\n",
    "✔ tibble  2.1.3     ✔ forcats 0.4.0\n",
    "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
    "✖ purrr::accumulate() masks foreach::accumulate()\n",
    "✖ tidyr::expand()     masks Matrix::expand()\n",
    "✖ dplyr::filter()     masks stats::filter()\n",
    "✖ dplyr::lag()        masks stats::lag()\n",
    "✖ purrr::lift()       masks caret::lift()\n",
    "✖ purrr::when()       masks foreach::when()\n",
    "                           \n",
    "    1. TRUE\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** First installation of plotmo:\n",
    "\n",
    "```R\n",
    "Loading required package: plotmo\n",
    "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
    "“there is no package called ‘plotmo’”Installing package into ‘/home/jovyan/R/x86_64-pc-linux-gnu-library/3.6’\n",
    "(as ‘lib’ is unspecified)\n",
    "also installing the dependencies ‘plotrix’, ‘TeachingDemos’\n",
    "\n",
    "Loading required package: plotmo\n",
    "Loading required package: Formula\n",
    "Loading required package: plotrix\n",
    "Loading required package: TeachingDemos\n",
    "\n",
    "    1. TRUE\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::There are two versions in mlbench\n",
    "# https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/data\n",
    "#::NOTE::2nd version comes with zero outliers as NAs\n",
    "utils::data(PimaIndiansDiabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::See first rows\n",
    "# https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/head\n",
    "#::NOTE::Some NAs show up!\n",
    "utils::head(PimaIndiansDiabetes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** The goal of this stage of our study is to get to know the data, their flaws and perhaps some clues to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#::GMG::PimaIndiansDiabetes2 (say, v2) has the outliers from \n",
    "#       glucose, pressure, triceps, insulin and mass as NAs\n",
    "str(PimaIndiansDiabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::NA counts yield the outliers (!)\n",
    "#::NOTE::Should I standardize some of the features ('glucose', 'pressure', 'triceps', 'insuline', 'mass')?\n",
    "#::NOTE::Should I do feature engineering on some of the features \n",
    "#        (i.e. age, p.e breaking it into intervals with labels)?\n",
    "summary(PimaIndiansDiabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#::GMG::Check for missing values ... I know they are marked witn 'NA' in mlbench 2nd dataset\n",
    "sapply(PimaIndiansDiabetes2, function(x) sum(is.na(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Now I test the imbalance of the dataset\n",
    "# http://metadatascience.com/2013/07/03/plotting-the-frequency-distribution-using-r/\n",
    "table(PimaIndiansDiabetes2$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "prop.table(table(PimaIndiansDiabetes2$diabetes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#::GMG::Plot the imbalance\n",
    "# https://www.statmethods.net/advgraphs/parameters.html\n",
    "# http://www.programmingr.com/content/positioning-charts-fig-and-fin/\n",
    "#::NOTE::I don't know how to make the fig smaller and the margins tight (!?)\n",
    "# https://blog.revolutionanalytics.com/2015/09/resizing-plots-in-the-r-kernel-for-jupyter-notebooks.html\n",
    "options(repr.plot.width=4, repr.plot.height=3)\n",
    "barplot(100*prop.table(table(PimaIndiansDiabetes2$diabetes)), \n",
    "        col = c('blue', 'red'),\n",
    "        ylim = c(0,70),\n",
    "        main = 'Outcome Imbalance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG:: (STHDA) Load the data and remove NAs\n",
    "#::NOTE::I've already attached that dataset earlier\n",
    "# data(\"PimaIndiansDiabetes2\", package = \"mlbench\")\n",
    "# https://stat.ethz.ch/R-manual/R-devel/library/stats/html/na.fail.html\n",
    "# https://www.statmethods.net/input/missingdata.html\n",
    "pima_clean <- stats::na.omit(PimaIndiansDiabetes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Check for missing values ... No NAs after cleaning with na.omit()\n",
    "sapply(pima_clean, function(x) sum(is.na(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "# https://dplyr.tidyverse.org/reference/sample.html\n",
    "#dplyr::sample_n(pima_clean, 3)\n",
    "#::NOTE::Downsize use of tidyverse to R core functions\n",
    "# https://www.rdocumentation.org/packages/utils/versions/3.6.1/topics/head\n",
    "utils::head(pima_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::Originally 768 Observations!!\n",
    "str(pima_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stratified Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::Make it reproducible\n",
    "set.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test set\n",
    "#::NOTE::STHDA uses caret to do the stratified split and dplyr %>% notation\n",
    "#        and I think It's a bit of overdoing it ...\n",
    "# https://topepo.github.io/caret/data-splitting.html\n",
    "# https://github.com/tidyverse/dplyr\n",
    "#training_samples <- pima_clean$diabetes %>% \n",
    "#  caret::createDataPartition(p = 0.8, list = FALSE)\n",
    "#::NOTE::... downsizing to caTools (that does a pretty good job, I think)\n",
    "msk <- caTools::sample.split(Y = pima_clean$diabetes, SplitRatio = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::Do the actual split with the indices just computed\n",
    "#train_data  <- pima_clean[training_samples, ]\n",
    "#test_data <- pima_clean[-training_samples, ]\n",
    "#::NOTE::downsizing to caTools\n",
    "train_data <- pima_clean[msk,]  # use output of sample.split to ...\n",
    "test_data  <- pima_clean[!msk,] # create train and test subse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::See the split datasets\n",
    "str(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Now I test the imbalance of the dataset\n",
    "# http://metadatascience.com/2013/07/03/plotting-the-frequency-distribution-using-r/\n",
    "table(train_data$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Just to be sure proportions are kept ...\n",
    "prop.table(table(train_data$diabetes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "str(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Now I test the imbalance of the dataset\n",
    "# http://metadatascience.com/2013/07/03/plotting-the-frequency-distribution-using-r/\n",
    "table(test_data$diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Just to be sure proportions are kept ...\n",
    "prop.table(table(test_data$diabetes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::First off, I use stats:glm() to fit a baseline Logistic Regression Model\n",
    "# https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/glm\n",
    "full_model <- stats::glm(\n",
    "    formula = diabetes ~., \n",
    "    data = train_data, \n",
    "    family = \"binomial\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#::GMG::Let's see a summary of the model I've just got\n",
    "# https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/summary.glm\n",
    "# https://www.youtube.com/watch?v=xl5dZo_BSJk\n",
    "# \n",
    "summary(full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Selecting the statistically significant variables in an R glm model\n",
    "# https://stackoverflow.com/questions/16153497/selecting-the-statistically-significant-variables-in-an-r-glm-model\n",
    "toselect.x <- summary(full_model)$coeff[-1,4] < 0.05 # credit to kith\n",
    "# select sig. variables\n",
    "relevant.x <- names(toselect.x)[toselect.x == TRUE] \n",
    "# formula with only sig variables\n",
    "sig.formula <- as.formula(paste(\"diabetes ~\",paste(relevant.x, collapse= \"+\")))\n",
    "cat('Relevant: ', relevant.x, '\\n')\n",
    "print (sig.formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::PENDING::** <span style=\"color:red\">Discussion here</span>\n",
    "\n",
    "**::GMG::** Help ...\n",
    "\n",
    "  - [Jeff Harmrick](https://www.youtube.com/watch?v=xl5dZo_BSJk) Understanding the Summary Output for a Logistic Regression in R, Published on Nov 14, 2012.\n",
    "  - [Cross-Validated](https://stats.stackexchange.com/questions/232548/r-how-are-the-significance-codes-determined-when-summarizing-a-logistic-regres) R - How are the significance codes determined when summarizing a logistic regression model?, Apr 13 2017 at 12:44\n",
    "  - [Cross-Validated](https://stats.stackexchange.com/questions/72258/how-to-interpret-the-significance-code) How to interpret the significance code?, Oct 9 2013 at 2:26\n",
    "  - [Cross_validated](https://stats.stackexchange.com/questions/60074/wald-test-for-logistic-regression) Wald test for logistic regression, May 26 2013 at 20:40 (Check out the [answer](https://stats.stackexchange.com/a/60083))\n",
    "  - [R Tutorial](http://www.r-tutor.com/elementary-statistics/logistic-regression/significance-test-logistic-regression) Significance Test for Logistic Regression\n",
    "  - [StatQuest](https://www.youtube.com/watch?v=yIYKR4sgzI8) Logistic Regression, by Josh Starmer Published on 5 Mar 2018\n",
    "  - [StatQuest](https://www.youtube.com/watch?v=vN5cNN2-HWE) Logistic Regression Details Pt1: Coefficients, by Josh Starmer, Published on Jun 4, 2018\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "simpler_model <- stats::glm(\n",
    "    formula = diabetes ~ glucose + mass + pedigree + age, \n",
    "    data = train_data, \n",
    "    family = \"binomial\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summary(simpler_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::PENDING::** <span style=\"color:red\">Discussion here</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Make predictions (from STHDA)\n",
    "# https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/predict.glm\n",
    "probabilities_full <- predict(\n",
    "    object = full_model, \n",
    "    newdata = test_data, \n",
    "    type = \"response\"\n",
    ")\n",
    "str(probabilities_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Make class predictions (from STHDA)\n",
    "#predicted_class_full <- ifelse(probabilities_full > 0.5, \"pos\", \"neg\")\n",
    "#::NOTE::I make the result a factor with levels that match the taget variable\n",
    "predicted_class_full <- as.factor(ifelse(probabilities_full > 0.5, 2, 1))\n",
    "levels(predicted_class_full) <- c('neg', 'pos')\n",
    "str(predicted_class_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "head(predicted_class_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::For the P/R curve I need a numeric 0/1 not class labels, I'm afraid\n",
    "str(predicted_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Check the simpler model (I don't expect mmuch ;)\n",
    "probabilities_simpler <- predict(\n",
    "    object = simpler_model, \n",
    "    newdata = test_data, \n",
    "    type = \"response\"\n",
    ")\n",
    "str(probabilities_simpler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# predicted_class_num <- ifelse(probabilities_full > 0.5, 1, 0)\n",
    "#::NOTE::I make the result a factor with levels that match the taget variable\n",
    "#predicted_class_simpler <- ifelse(probabilities_simpler > 0.5, \"pos\", \"neg\")\n",
    "predicted_class_simpler <- as.factor(ifelse(probabilities_simpler > 0.5, 2, 1))\n",
    "levels(predicted_class_simpler) <- c('neg', 'pos')\n",
    "str(predicted_class_simpler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "head(predicted_class_simpler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### evaluation: confusion matrix and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::'Raw' Model accuracy\n",
    "observed_classes <- test_data$diabetes\n",
    "str(observed_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "head(observed_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat('Accuracy Full Model: ', \n",
    "    mean(predicted_class_full == observed_classes),\n",
    "   '\\nAccuracy Simpler Model: ', \n",
    "    mean(predicted_class_simpler == observed_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Confusion Matrix AND metrics\n",
    "# https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/table\n",
    "# https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "# https://stackoverflow.com/questions/33621592/sum-of-antidiagonal-of-a-matrix\n",
    "# https://stackoverflow.com/questions/14577412/how-to-convert-variable-object-name-into-string\n",
    "confusion_matrix <- function (true, pred, beta = 1) {\n",
    "    t <- table(true, pred)\n",
    "    p <- t[2,2]/sum(t[,2])  #::NOTE::I assume 'positive class' is class2 (whatever it is)\n",
    "    r <- t[2,2]/sum(t[2,])  #::NOTE::I assume 'positive class' is class2 (whatever it is)\n",
    "    s <- t[1,1]/sum(t[1,])\n",
    "    list(cm = t,\n",
    "         accuracy = sum(diag(t))/sum(t), \n",
    "         precision = p,     #::NOTE::Precision of 'positive class'    \n",
    "         recall = r,        #::NOTE::it's also called sensitivity ('positive class again')\n",
    "         f1 = (1 + beta^2) * p * r/((beta^2 * p) + r),  #::NOTA::beta = 1 -> equal importance (class1, class2)\n",
    "         specificity = s,   #::NOTE::proportion of actual 'negative class' that are detected as such\n",
    "         balanced_accuracy = (r + s) / 2\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Confusion Matrix\n",
    "cf_full <- confusion_matrix(true = test_data$diabetes, pred = predicted_class_full)\n",
    "print(cf_full$cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Confusion Matrix of the 'simpler' model ... no difference!\n",
    "cf_simpler <- confusion_matrix(true = test_data$diabetes, pred = predicted_class_simpler)\n",
    "print(cf_simpler$cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG:Pretty print the model metrics as a table/data frame/...\n",
    "# https://cran.r-project.org/web/packages/tibble/vignettes/tibble.html\n",
    "# tibble::tibble(metric = names(cf[-1]), pima_logit = cf[-1])\n",
    "# https://www.r-bloggers.com/converting-a-list-to-a-data-frame/\n",
    "#as.data.frame.list(x = cf[-1], row.names = c('pima_logit'))\n",
    "# https://stackoverflow.com/questions/32059798/list-of-named-lists-to-data-frame\n",
    "# https://stackoverflow.com/questions/10432993/named-list-to-from-data-frame\n",
    "# https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/as.data.frame\n",
    "# https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/names\n",
    "#::NOTE::sensitivity (pos class) is recall\n",
    "results <- as.data.frame(t(sapply(list(cf_full[-1], cf_simpler[-1]), rbind)),\n",
    "                         row.names = c('full_model', 'simpler_model')\n",
    ")\n",
    "colnames(results) <- names(cf_full[-1])\n",
    "# https://stackoverflow.com/questions/23217520/limiting-the-number-of-decimals-in-a-dataframe-r\n",
    "print(results, digits = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### evaluation: P/R Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Compute the curve\n",
    "# https://cran.r-project.org/web/packages/PRROC/PRROC.pdf pp. 7\n",
    "# Soft-labeled classification problems:\n",
    "# Each data point belongs to both of the two classes with a certain probability, where for each datapoint, these \n",
    "# two probabilities add up to 1.  In this case, the classification scores for all data points need to be provided \n",
    "# only once as scores.class0 and only the positive/foreground weights for each data point need to be provided in\n",
    "# weights.class0, while the converse probability for the negative class is automatically set to \n",
    "# weights.class1 = 1.0 - weights.class0.\n",
    "pr_curve_full <- PRROC::pr.curve(\n",
    "                scores.class0 = predicted_class_num, \n",
    "                weights.class0 = probabilities_full,\n",
    "                min.compute = T, \n",
    "                max.compute = T,\n",
    "                rand.compute = T,\n",
    "                curve = T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::See all things within the pr_1 object\n",
    "print(pr_curve_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Plot the curve\n",
    "# https://cran.r-project.org/web/packages/PRROC/PRROC.pdf pp 3-4\n",
    "options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "plot(pr_curve_full, rand.plot = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::PENDING::** <span style=\"color:red\">Discussion here</span>\n",
    "\n",
    "**::NOTE::** Is this right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model with Lasso (L1 regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### features/target split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::The R function model.matrix() helps to create the matrix of predictors and also automatically \n",
    "#        converts categorical predictors to appropriate dummy variables, which is required for the glmnet() \n",
    "#        function.\n",
    "# https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/model.matrix\n",
    "# Dumy code categorical predictor variables\n",
    "X_train <- stats::model.matrix(diabetes~., train_data)[,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Convert the outcome (class) to a numerical variable\n",
    "y_train <- ifelse(train_data$diabetes == \"pos\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::Check the results (a named 2D matrix of features)\n",
    "str(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::Check the results (a numeric 1/0 vector)\n",
    "str(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Do the same with test, of course\n",
    "X_test <- model.matrix(diabetes ~., test_data)[,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "str(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_test <- ifelse(test_data$diabetes == \"pos\", 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "str(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### computing best lambda with cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::Make it reproducible\n",
    "set.seed(123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::In penalized regression, you need to specify a constant lambda to adjust the amount of \n",
    "#        the coefficient shrinkage. The best lambda for your data, can be defined as the lambda \n",
    "#        that minimize the cross-validation prediction error rate. This can be determined automatically \n",
    "#        using the function cv.glmnet()\n",
    "# https://www.rdocumentation.org/packages/glmnet/versions/2.0-18/topics/cv.glmnet\n",
    "#::NOTE::compute lasso regression by specifying the option alpha = 1\n",
    "# Find the optimal value of lambda that minimizes the cross-validation error:\n",
    "cv_lasso <- glmnet::cv.glmnet(x = X_train, \n",
    "                              y = y_train, \n",
    "                              alpha = 1,\n",
    "                              type.measure = 'class', #::NOTE::misclassification error\n",
    "                              family = 'binomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE::See graphically the best lambda\n",
    "options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "plot(cv_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The plot displays the cross-validation *misclassification error* according to the log of lambda. The *left dashed vertical line* indicates that the log of the optimal value of lambda `lambda.min` is approximately -3, which is the one that minimizes the prediction error. This lambda value will give the most accurate model. The exact value of lambda can be viewed as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat('Optimum lambda: ', cv_lasso$lambda.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generally, the purpose of regularization is to balance accuracy and simplicity. This means, a model with the smallest number of predictors that also gives a good accuracy. To this end, the function `cv.glmnet()` finds also the value of lambda that gives the simplest model but also lies within one standard error of the optimal value of lambda. This value is called `lambda.1se`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat('Simplest model within one sd of lambda optimum: ', cv_lasso$lambda.1se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using lambda.min as the best lambda, gives the following regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/coef (?)\n",
    "stats::coef(cv_lasso, cv_lasso$lambda.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::NOTE:the \"native\" glmnet packaage coef() (same result)\n",
    "glmnet::coef.cv.glmnet(cv_lasso, cv_lasso$lambda.min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using `lambda.1se` as the best lambda, gives the following regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "coef(cv_lasso, cv_lasso$lambda.1se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::PENDING::** <span style=\"color:red\">Discussion here</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Plot the shrinkage of the coefficients\n",
    "# https://stackoverflow.com/questions/36656752/plotting-cv-glmnet-in-r~\n",
    "#::NOTE::How do I put the feature names on that plot?\n",
    "# https://stats.stackexchange.com/questions/133873/lasso-plot-label-lines-with-names-using-glmnet\n",
    "# https://www.rdocumentation.org/packages/glmnet/versions/2.0-18/topics/plot.glmnet\n",
    "options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "plot(cv_lasso$glmnet.fit, \"lambda\", label = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### better coeff shrinkage plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Adhoc function intermediate fix\n",
    "#::NOTE::Adapted from\n",
    "# https://stackoverflow.com/a/43327018\n",
    "lbs_fun <- function(lra, ...) {\n",
    "  fit <- lra$glmnet.fit\n",
    "\n",
    "  L=which(fit$lambda==lra$lambda.min)\n",
    "\n",
    "  ystart <- sort(fit$beta[abs(fit$beta[,L])>0,L])\n",
    "  labs <- names(ystart)\n",
    "  #::NOTE:: max gap between biggest and smallest coefs at smallest lambda\n",
    "  #         i.e., 64th (more than that yields an error ... given 100th lambda\n",
    "  #         in original!)\n",
    "  r <- range(fit$beta[,64]) \n",
    "  yfin <- seq(r[1],r[2],length=length(ystart))\n",
    "\n",
    "  xstart<- log(lra$lambda.min)\n",
    "  xfin <- xstart+1\n",
    "\n",
    "  text(xfin+0.3,yfin,labels=labs,...)\n",
    "  segments(xstart,ystart,xfin,yfin)\n",
    "  # https://www.r-bloggers.com/r-add-vertical-line-to-a-plot/\n",
    "  # https://www.statmethods.net/advgraphs/parameters.html\n",
    "  abline(v = log(lra$lambda.min), col = 'darkgreen', lty = 3, lwd = 2) \n",
    "  text(log(lra$lambda.min) - 1, r[2], col = 'darkgreen', 'chosen lambda')\n",
    "  segments(log(lra$lambda.min), r[2] - 0.3 , log(lra$lambda.min) - 1, r[2], col = 'darkgreen')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Let's what it looks like ...\n",
    "options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "plot(cv_lasso$glmnet.fit, label=FALSE, xvar=\"lambda\")\n",
    "lbs_fun(cv_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### better coeff shrinkage plot with plotmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::It's weird with those numeric labels (plot.glmnet)ç\n",
    "#::NOTE::It seems there's an alternative with adhoc plotmo package\n",
    "# https://stackoverflow.com/questions/30560689/adding-labels-on-curves-in-glmnet-plot-in-r\n",
    "# https://stats.stackexchange.com/questions/133873/lasso-plot-label-lines-with-names-using-glmnet\n",
    "# https://cran.r-project.org/web/packages/plotmo/plotmo.pdf#page=17\n",
    "# plot_glmnet: Plot a glmnet model (Plot the coefficient paths of a glmnet model.)\n",
    "# http://www.milbo.org/doc/plotres-notes.pdf#page=7\n",
    "options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "plotmo::plot_glmnet(x = cv_lasso$glmnet.fit, xvar = 'lambda', label = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** In STHDA a new model is trained with the best lambdas found out with the CV approach. Is it really necessary to do that? Does not `cv.glmnet()` already have provided such a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Cross-validated model with ... lambda.min or lambda.1se?\n",
    "# https://www.rdocumentation.org/packages/glmnet/versions/2.0-18/topics/cv.glmnet\n",
    "# https://cran.r-project.org/web/packages/glmnet/glmnet.pdf pp. 6, \n",
    "# glmnet.fit: a fitted glmnet object for the full data.\n",
    "str(cv_lasso$glmnet.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Let's see the regression coefficients\n",
    "coef(cv_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG:: (STHDA) Fit the final model on the training data and lambda.min\n",
    "model_lambda_min <- glmnet(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    alpha = 1,                     #::NOTE:: L1 LASSO regularization\n",
    "    family = \"binomial\",\n",
    "    lambda = cv_lasso$lambda.min   #::NOTE::Lambda min for L1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Display regression coefficients\n",
    "coef(model_lambda_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Final model with lambda.1se (this is the only way to get it this time)\n",
    "model_lambda_1se <- glmnet(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    alpha = 1, \n",
    "    family = \"binomial\",\n",
    "    lambda = cv_lasso$lambda.1se\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Display regression coefficients\n",
    "coef(model_lambda_1se)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::GMG::** Looking at the `coef()` from `cv_lasso` and `model_lambda_1se` I conclude that the model returned as value (`cv_lasso$glmnet.fit` reported in the documentation) is the simpler regularized L1 model indeed! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Make predictions on the test data using the new trained model (STHDA)\n",
    "probabilities_lambda_min <- predict(model_lambda_min, newx = X_test)\n",
    "predicted_classes_lambda_min <- ifelse(probabilities_lambda_min > 0.5, \"pos\", \"neg\")\n",
    "#::NOTE::For the P/R Curve function\n",
    "predicted_classes_lambda_min_num <- ifelse(probabilities_lambda_min > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Make prediction on test data on the test data using the lambda 1se model\n",
    "probabilities_1se <- predict(model_lambda_1se, newx = X_test)\n",
    "predicted_classes_1se <- ifelse(probabilities_1se > 0.5, \"pos\", \"neg\")\n",
    "predicted_classes_1se_num <- ifelse(probabilities_1se > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Predict using the cross-validated model\n",
    "# https://cran.r-project.org/web/packages/glmnet/glmnet.pdf pp. 18\n",
    "# https://www.rdocumentation.org/packages/glmnet/versions/2.0-18/topics/predict.cv.glmnet\n",
    "# This function makes predictions from a cross-validated glmnet model, using the stored \"glmnet.fit\" object, \n",
    "# and the optimal value chosen for lambda.\n",
    "#::NOTE::So as long as I don't want the simpler model with lambda.1se I can predict with this!\n",
    "probabilities_cv_lasso <- predict(cv_lasso, newx = X_test)\n",
    "predicted_classes_cv_lasso <- ifelse(probabilities_cv_lasso > 0.5, \"pos\", \"neg\")\n",
    "#::NOTE::For the P/R Curve function\n",
    "predicted_classes_cv_lasso_num <- ifelse(probabilities_cv_lasso > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "probabilities_lambda_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### evaluation: confusion matrix and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::'Raw' Model accuracy\n",
    "#::NOTE::observed_classes <- test_data$diabetes\n",
    "cat('Accuracy L1 Lambda Min Model: ', \n",
    "    mean(predicted_classes_lambda_min == observed_classes),\n",
    "   '\\nAccuracy L1 Lambda 1se (Simpler) Model: ', \n",
    "    mean(predicted_classes_1se == observed_classes),\n",
    "   '\\nAccuracy CV Lasso Model: ', \n",
    "    mean(predicted_classes_cv_lasso == observed_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Confusion Matrix\n",
    "cf_lambda_min <- confusion_matrix(true = test_data$diabetes, pred = predicted_classes_lambda_min)\n",
    "#cf_lambda_min <- MLmetrics::ConfusionMatrix(y_true = test_data$diabetes, \n",
    "#                                            y_pred = predicted_classes_lambda_min)\n",
    "#print(cf_lambda_min)\n",
    "print(cf_lambda_min$cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Confusion Matrix\n",
    "cf_lambda_1se <- confusion_matrix(true = test_data$diabetes, pred = predicted_classes_1se)\n",
    "print(cf_lambda_1se$cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Confusion Matrix\n",
    "cf_cv_lasso <- confusion_matrix(true = test_data$diabetes, pred = predicted_classes_cv_lasso)\n",
    "print(cf_cv_lasso$cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Metrics\n",
    "# https://stackoverflow.com/questions/3443687/formatting-decimal-places-in-r\n",
    "cat(sep='', \"Accuracy: \", format(round(cf_full$accuracy, 2), nsmall = 2), \n",
    "    \", Balanced Accuracy \", format(round(cf_full$balanced_accuracy, 2), nsmall = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat(sep = '', \"Precision: \", format(round(cf_full$precision, 2), nsmall = 2), \n",
    "    \", Recall: \", format(round(cf_full$recall, 2), nsmall = 2),\n",
    "    \", Fl: \", format(round(cf_full$f1, 2), nsmall = 2)\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cat(sep='', \"Specificity: \", format(round(cf_full$specificity, 2), nsmall = 2), \n",
    "    \", Sensitivity \", format(round(cf_full$recall, 2), nsmall = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Confusion Matrix of the 'simpler' model ... no difference!\n",
    "cf_simpler <- confusion_matrix(true = test_data$diabetes, pred = predicted_class_simpler)\n",
    "print(cf_simpler$cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### evaluation: P/R Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Compute the curve\n",
    "# https://cran.r-project.org/web/packages/PRROC/PRROC.pdf pp. 7\n",
    "# Soft-labeled classification problems:\n",
    "# Each data point belongs to both of the two classes with a certain probability, where for each datapoint, these \n",
    "# two probabilities add up to 1.  In this case, the classification scores for all data points need to be provided \n",
    "# only once as scores.class0 and only the positive/foreground weights for each data point need to be provided in\n",
    "# weights.class0, while the converse probability for the negative class is automatically set to \n",
    "# weights.class1 = 1.0 - weights.class0.\n",
    "pr_curve_full <- PRROC::pr.curve(\n",
    "                scores.class0 = predicted_class_num, \n",
    "                weights.class0 = probabilities_full,\n",
    "                min.compute = T, \n",
    "                max.compute = T,\n",
    "                rand.compute = T,\n",
    "                curve = T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::See all things within the pr_1 object\n",
    "print(pr_curve_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#::GMG::Plot the curve\n",
    "# https://cran.r-project.org/web/packages/PRROC/PRROC.pdf pp 3-4\n",
    "options(repr.plot.width = 8, repr.plot.height = 6)\n",
    "plot(pr_curve_full, rand.plot = T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**::PENDING::** <span style=\"color:red\">Discussion here</span>\n",
    "\n",
    "**::NOTE::** Is this right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
